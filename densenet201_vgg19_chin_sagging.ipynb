{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SurinSeong/FinalPJT/blob/main/densenet201_vgg19_chin_sagging.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# train, val, eval 구성 변경\n",
        "* train : 증강이미지만 (train) 2490개\n",
        "* test(val + eval) : 원본이미지만 5:5 분할해서 val, eval로 나누기 965개씩\n",
        "\n",
        "* 추가\n",
        "    * weight_decay, scheduler(cosine annealing 추가)\n",
        "\n",
        "1. 1st\n",
        "        lr = 1e-4, batch_size = 16 (train), weight_decay = 1e-5\n",
        "        (0.0878) ensemble_chin_0828\n",
        "        Evaluation Accuracy: 95.34% (batch=16)\n",
        "                    precision    recall  f1-score   support\n",
        "\n",
        "                0       0.97      0.96      0.96       645\n",
        "                1       0.92      0.94      0.93       320\n",
        "\n",
        "         accuracy                           0.95       965\n",
        "        macro avg       0.95      0.95      0.95       965\n",
        "        weighted avg    0.95      0.95      0.95       965"
      ],
      "metadata": {
        "id": "dCB38J98Agfz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HlaxTZDsqyhV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3bc6e7a-1e5d-4f7e-ad60-0ac1928ed7dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.models as models\n",
        "from torchvision import models, transforms\n",
        "from torch.utils.data import DataLoader, Dataset, ConcatDataset\n",
        "from torchvision.models import DenseNet201_Weights, VGG19_Weights\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import cv2\n",
        "\n",
        "import numpy as np\n",
        "import time\n",
        "import os"
      ],
      "metadata": {
        "id": "0TM2fYuBq6nY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 초기 가중치 설정"
      ],
      "metadata": {
        "id": "6KkSKdGLrGIk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 클래스의 샘플 수\n",
        "chin_sagging_class_counts = np.array([632, 333])\n",
        "\n",
        "# 전체 샘플 수\n",
        "total_samples = 965\n",
        "\n",
        "# 클래스 비율에 기반한 가중치 계산\n",
        "chin_sagging_class_weights = total_samples / (len(chin_sagging_class_counts) * chin_sagging_class_counts)\n",
        "\n",
        "# 가중치를 텐서로 변환\n",
        "chin_sagging_class_weights = torch.tensor(chin_sagging_class_weights, dtype=torch.float32).to(device)\n",
        "\n",
        "print(chin_sagging_class_weights)"
      ],
      "metadata": {
        "id": "tDR1UQALq8Pb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05087af5-5c04-47e6-b268-6dba0ebc70b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.7634, 1.4489])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 모델 구축"
      ],
      "metadata": {
        "id": "NBLlRnQjrIfv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DenseNet201_VGG19_Ensemble(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(DenseNet201_VGG19_Ensemble, self).__init__()\n",
        "\n",
        "        # DenseNet201 정의\n",
        "        self.densenet = models.densenet201(weights=DenseNet201_Weights.DEFAULT)\n",
        "        densenet_features = self.densenet.classifier.in_features\n",
        "        self.densenet.classifier = nn.Identity() # 최종 분류기 제거\n",
        "\n",
        "        # VGG19 정의\n",
        "        self.vgg19 = models.vgg19(weights=VGG19_Weights.DEFAULT)\n",
        "        vgg19_features = self.vgg19.classifier[0].in_features\n",
        "        self.vgg19.classifier = nn.Identity() # 최종 분류기 제거\n",
        "\n",
        "        # 두 모델의 특징을 결합하는 계층\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(densenet_features + vgg19_features, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # DenseNet201 특징 추출\n",
        "        densenet_features = self.densenet(x)\n",
        "        # VGG19 특징 추출\n",
        "        vgg19_features = self.vgg19(x)\n",
        "        # 두 특징 결합\n",
        "        combined_features = torch.cat((densenet_features, vgg19_features), dim=1)\n",
        "\n",
        "        # 최종 분류\n",
        "        output = self.classifier(combined_features)\n",
        "        return output"
      ],
      "metadata": {
        "id": "rHYsKcN1rJ46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "####################\n",
        "# 모델 학습 클래스 #\n",
        "####################\n",
        "\n",
        "## 모델 훈련 클래스 ##\n",
        "class ModelTrainer:\n",
        "    def __init__(self, model, train_loader, val_loader, criterion, optimizer, scheduler, device):\n",
        "        self.model = model.to(device)\n",
        "        self.train_loader = train_loader\n",
        "        self.val_loader = val_loader\n",
        "        self.criterion = criterion\n",
        "        self.optimizer = optimizer\n",
        "        self.scheduler = scheduler\n",
        "        self.device = device\n",
        "\n",
        "    def train_and_val(self, num_epochs, save_dir, today):\n",
        "        early_stopping = EarlyStopping(patience=10, verbose=True)\n",
        "\n",
        "        self.model.train()\n",
        "\n",
        "        valid_loss_min = np.inf\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            start = time.time()\n",
        "\n",
        "            train_loss = 0.0\n",
        "            train_correct = 0\n",
        "            train_total = 0\n",
        "\n",
        "            ## Train Mode ##\n",
        "            for images, labels in self.train_loader:\n",
        "                images, labels = images.to(self.device), labels.to(self.device)\n",
        "\n",
        "                self.optimizer.zero_grad()\n",
        "                outputs = self.model(images)\n",
        "                _, train_preds = torch.max(outputs, 1) # 예측 클래스 얻기\n",
        "                train_total += labels.size(0)\n",
        "                train_correct += (train_preds == labels).sum().item()\n",
        "\n",
        "                loss = self.criterion(outputs, labels)\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "\n",
        "                train_loss += loss.item()\n",
        "\n",
        "            train_accuracy = train_correct / train_total * 100\n",
        "\n",
        "            self.scheduler.step()\n",
        "            current_lr = self.scheduler.get_last_lr()[0]\n",
        "\n",
        "            # loss 값, accuracy 값 출력\n",
        "            print(f'Epoch [{epoch+1}/{num_epochs}], Time: {float(time.time()-start):.1f}s, Learning rate: {current_lr}\\nTraining Loss: {train_loss/len(self.train_loader):.4f}, Accuracy: {train_accuracy:.2f}%')\n",
        "\n",
        "            ## Validation Mode ##\n",
        "            self.model.eval()\n",
        "\n",
        "            valid_correct = 0\n",
        "            valid_total = 0\n",
        "            valid_loss = 0.0\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for images, labels in self.val_loader:\n",
        "                    images, labels = images.to(self.device), labels.to(self.device)\n",
        "\n",
        "                    outputs = self.model(images)\n",
        "                    _, preds = torch.max(outputs, 1) # 예측 클래스 얻기\n",
        "                    valid_total += labels.size(0)\n",
        "                    valid_correct += (preds == labels).sum().item()\n",
        "\n",
        "                    loss = self.criterion(outputs, labels)\n",
        "                    valid_loss += loss.item()\n",
        "\n",
        "            valid_loss /= len(self.val_loader)\n",
        "            valid_accuracy = valid_correct / valid_total * 100\n",
        "\n",
        "            # loss 값 출력\n",
        "            print(f\"Validation Loss: {valid_loss:.4f}, Accuracy: {valid_accuracy:.2f}%\")\n",
        "\n",
        "            # Early stopping 체크\n",
        "            early_stopping(valid_loss)\n",
        "\n",
        "            if early_stopping.early_stop:\n",
        "                print('Early stopping')\n",
        "                break\n",
        "\n",
        "            # 모델 저장\n",
        "            if valid_loss <= valid_loss_min:\n",
        "                print(f'Validation loss decreased ({valid_loss_min:.4f} --> {valid_loss:.4f}). Saving model...')\n",
        "                torch.save(model.state_dict(), f'{save_dir}ensemble_chin_{today}.pt')\n",
        "                valid_loss_min = valid_loss\n",
        "\n",
        "####################\n",
        "# 모델 평가 클래스 #\n",
        "####################\n",
        "\n",
        "# 모델 평가\n",
        "class ModelEvaluator:\n",
        "    def __init__(self, best_model_state_dict, eval_loader, criterion, device):\n",
        "        self.best_model_state_dict = best_model_state_dict\n",
        "        self.eval_loader = eval_loader\n",
        "        self.criterion = criterion\n",
        "        self.device = device\n",
        "\n",
        "    def evaluate(self):\n",
        "        model = DenseNet201_VGG19_Ensemble(num_classes)\n",
        "        model.load_state_dict(self.best_model_state_dict)\n",
        "\n",
        "        model.to(self.device).eval()\n",
        "\n",
        "        eval_correct = 0\n",
        "        eval_total = 0\n",
        "\n",
        "        eval_true = []\n",
        "        eval_pred = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for images, labels in self.eval_loader:\n",
        "                images, labels = images.to(self.device), labels.to(self.device)\n",
        "\n",
        "                outputs = model(images)\n",
        "                _, eval_preds = torch.max(outputs, 1)\n",
        "\n",
        "                eval_true.extend(labels.cpu().numpy())\n",
        "                eval_pred.extend(eval_preds.cpu().numpy())\n",
        "\n",
        "                eval_total += labels.size(0)\n",
        "                eval_correct += (eval_preds == labels).sum().item()\n",
        "\n",
        "        eval_accuracy = eval_correct / eval_total * 100\n",
        "\n",
        "        print(f\"Evaluation Accuracy: {eval_accuracy:.2f}%\")\n",
        "\n",
        "        return eval_pred, eval_true\n",
        "\n",
        "#############\n",
        "# 조기 종료 #\n",
        "#############\n",
        "\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=5, verbose=False, delta=0):\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.delta = delta\n",
        "\n",
        "    def __call__(self, val_loss):\n",
        "        score = -val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            if self.verbose:\n",
        "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.counter = 0\n"
      ],
      "metadata": {
        "id": "5fSRjqkprdjg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##########################\n",
        "# 데이터셋 및 데이터로더 #\n",
        "##########################\n",
        "\n",
        "## 데이터셋 정의 ##\n",
        "class AnnotationDataset(Dataset):\n",
        "    def __init__(self, image_dirs, csv_file, annotation, transform=None):\n",
        "\n",
        "        if csv_file is not None and image_dirs is not None:\n",
        "            # CSV 파일 로드 및 레이블 설정\n",
        "            self.image_paths = []\n",
        "            self.labels = []\n",
        "            self.transform = transform\n",
        "\n",
        "            data = pd.read_csv(csv_file)\n",
        "\n",
        "            for image_dir in image_dirs:\n",
        "                for image_file in os.listdir(image_dir):\n",
        "                    if image_file.endswith('.jpg'):\n",
        "                        image_path = os.path.join(image_dir, image_file)\n",
        "                        image_id = image_file.split('_')[0]\n",
        "                        label_data = data[data['ID'] == int(image_id)][annotation].values\n",
        "                        if len(label_data) > 0:\n",
        "                            label = label_data[0]\n",
        "                            self.image_paths.append(image_path)\n",
        "                            self.labels.append(label)\n",
        "\n",
        "            # 넘파이 배열로 변경\n",
        "            self.image_paths = np.array(self.image_paths)\n",
        "            self.labels = np.array(self.labels)\n",
        "\n",
        "        else:\n",
        "            raise ValueError('Both csv file and image folders must be provided.')\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_paths[idx]\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label"
      ],
      "metadata": {
        "id": "HfQKY0Jwrh8b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 이미지 전처리 파이프라인\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)), # ResNet50의 입력 크기에 맞게 조정\n",
        "    transforms.ToTensor(),  # 텐서로 변환\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # 정규화\n",
        "])"
      ],
      "metadata": {
        "id": "9Juhv2XmrlKB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## PATH ##\n",
        "data_dir = '/content/drive/MyDrive/Final_project_2조/02_2. 전처리 및 EDA_이미지/'\n",
        "\n",
        "csv_file = data_dir + 'data/annotation/annotation_class2.csv'\n",
        "\n",
        "## train PATH ##\n",
        "train_image_dirs = [os.path.join(data_dir + 'data/image/Orientation/train/chin', folder) for folder in os.listdir(data_dir + 'data/image/Orientation/train/chin') if not (folder.startswith('.') or folder in ['chin_origin', 'smart_pad'])]\n",
        "\n",
        "## test PATH ##\n",
        "val_image_dirs = [data_dir + 'data/image/Orientation/train/chin/chin_origin',\n",
        "                  data_dir + 'data/image/Orientation/val/chin/chin_origin',\n",
        "                  data_dir + 'data/image/Orientation/train/chin/smart_pad',\n",
        "                  data_dir + 'data/image/Orientation/val/chin/smart_pad']\n",
        "\n",
        "## save PATH ##\n",
        "save_dir = data_dir + '수린님/ensemble/model/'"
      ],
      "metadata": {
        "id": "N52WyqU2rnfC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_image_dirs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VHsjKErcW5XU",
        "outputId": "c61570f2-3476-4e0d-dca4-d633ecbd9d92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/drive/MyDrive/Final_project_2조/02_2. 전처리 및 EDA_이미지/data/image/Orientation/train/chin/horizon',\n",
              " '/content/drive/MyDrive/Final_project_2조/02_2. 전처리 및 EDA_이미지/data/image/Orientation/train/chin/color_minus_10',\n",
              " '/content/drive/MyDrive/Final_project_2조/02_2. 전처리 및 EDA_이미지/data/image/Orientation/train/chin/color_plus_10',\n",
              " '/content/drive/MyDrive/Final_project_2조/02_2. 전처리 및 EDA_이미지/data/image/Orientation/train/chin/rotation_plus_10',\n",
              " '/content/drive/MyDrive/Final_project_2조/02_2. 전처리 및 EDA_이미지/data/image/Orientation/train/chin/rotation_minus_10',\n",
              " '/content/drive/MyDrive/Final_project_2조/02_2. 전처리 및 EDA_이미지/data/image/Orientation/train/chin/background']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "## train 데이터셋 준비 ##\n",
        "train_dataset = AnnotationDataset(train_image_dirs, csv_file, annotation='chin_sagging', transform=transform)\n",
        "\n",
        "## valid / test 데이터셋 준비 ##\n",
        "test_dataset = AnnotationDataset(val_image_dirs, csv_file, annotation='chin_sagging', transform=transform)\n",
        "\n",
        "# valid / eval 데이터셋 나누기\n",
        "indices = np.arange(len(test_dataset))\n",
        "valid_indices, eval_indices = train_test_split(indices, test_size=0.5, random_state=42)\n",
        "\n",
        "valid_subset = torch.utils.data.Subset(test_dataset, valid_indices)\n",
        "eval_subset = torch.utils.data.Subset(test_dataset, eval_indices)\n",
        "\n",
        "# 개수 확인\n",
        "len(train_dataset), len(test_dataset), len(valid_subset), len(eval_subset)"
      ],
      "metadata": {
        "id": "uYowt_mJr7Ex",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7f992e1-7c1f-4c4a-c3c2-98679734bc3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3086, 1930, 965, 965)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#####################\n",
        "# 모델 훈련 및 평가 #\n",
        "#####################\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "num_classes = 2\n",
        "lr = 1e-4\n",
        "batch_size = 16\n",
        "weight_decay = 1e-5\n",
        "num_epochs = 50\n",
        "today = '0828'\n",
        "\n",
        "# DataLoader\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "valid_loader = DataLoader(valid_subset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "model = DenseNet201_VGG19_Ensemble(num_classes=num_classes)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(weight=chin_sagging_class_weights)  # 손실 함수 + 초기 가중치 설정\n",
        "optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay) # 최적화\n",
        "scheduler = scheduler = CosineAnnealingLR(optimizer, T_max=20, eta_min=1e-6)\n",
        "\n",
        "trainer = ModelTrainer(model, train_loader, valid_loader, criterion, optimizer, scheduler, device)\n",
        "trainer.train_and_val(num_epochs=num_epochs, save_dir=save_dir, today=today)\n"
      ],
      "metadata": {
        "id": "C71VEspyr-T4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0805ff21-df56-4605-eb17-b3480922c50e",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/densenet201-c1103571.pth\" to /root/.cache/torch/hub/checkpoints/densenet201-c1103571.pth\n",
            "100%|██████████| 77.4M/77.4M [00:00<00:00, 158MB/s]\n",
            "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n",
            "100%|██████████| 548M/548M [00:06<00:00, 84.8MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/50], Time: 1067.2s, Learning rate: 9.939057285945933e-05\n",
            "Training Loss: 0.4028, Accuracy: 80.98%\n",
            "Validation Loss: 0.1949, Accuracy: 91.81%\n",
            "Validation loss decreased (inf --> 0.1949). Saving model...\n",
            "Epoch [2/50], Time: 132.9s, Learning rate: 9.757729755661012e-05\n",
            "Training Loss: 0.4969, Accuracy: 77.54%\n",
            "Validation Loss: 0.3087, Accuracy: 89.64%\n",
            "EarlyStopping counter: 1 out of 10\n",
            "Epoch [3/50], Time: 132.5s, Learning rate: 9.460482294732422e-05\n",
            "Training Loss: 0.2727, Accuracy: 89.24%\n",
            "Validation Loss: 0.1812, Accuracy: 93.06%\n",
            "Validation loss decreased (0.1949 --> 0.1812). Saving model...\n",
            "Epoch [4/50], Time: 133.2s, Learning rate: 9.054634122155992e-05\n",
            "Training Loss: 0.1843, Accuracy: 92.77%\n",
            "Validation Loss: 0.1497, Accuracy: 94.09%\n",
            "Validation loss decreased (0.1812 --> 0.1497). Saving model...\n",
            "Epoch [5/50], Time: 133.6s, Learning rate: 8.550178566873411e-05\n",
            "Training Loss: 0.1526, Accuracy: 93.71%\n",
            "Validation Loss: 0.2004, Accuracy: 91.50%\n",
            "EarlyStopping counter: 1 out of 10\n",
            "Epoch [6/50], Time: 132.8s, Learning rate: 7.959536998847744e-05\n",
            "Training Loss: 0.1221, Accuracy: 94.98%\n",
            "Validation Loss: 0.2665, Accuracy: 90.78%\n",
            "EarlyStopping counter: 2 out of 10\n",
            "Epoch [7/50], Time: 132.8s, Learning rate: 7.297252973710758e-05\n",
            "Training Loss: 0.0832, Accuracy: 97.21%\n",
            "Validation Loss: 0.3283, Accuracy: 93.68%\n",
            "EarlyStopping counter: 3 out of 10\n",
            "Epoch [8/50], Time: 134.0s, Learning rate: 6.579634122155991e-05\n",
            "Training Loss: 0.0706, Accuracy: 96.82%\n",
            "Validation Loss: 0.1653, Accuracy: 96.17%\n",
            "EarlyStopping counter: 4 out of 10\n",
            "Epoch [9/50], Time: 133.6s, Learning rate: 5.824350601949144e-05\n",
            "Training Loss: 0.0339, Accuracy: 98.80%\n",
            "Validation Loss: 0.1395, Accuracy: 93.58%\n",
            "Validation loss decreased (0.1497 --> 0.1395). Saving model...\n",
            "Epoch [10/50], Time: 134.4s, Learning rate: 5.050000000000001e-05\n",
            "Training Loss: 0.0226, Accuracy: 99.25%\n",
            "Validation Loss: 0.1452, Accuracy: 97.41%\n",
            "EarlyStopping counter: 1 out of 10\n",
            "Epoch [11/50], Time: 133.2s, Learning rate: 4.275649398050859e-05\n",
            "Training Loss: 0.0223, Accuracy: 99.48%\n",
            "Validation Loss: 0.0878, Accuracy: 97.31%\n",
            "Validation loss decreased (0.1395 --> 0.0878). Saving model...\n",
            "Epoch [12/50], Time: 134.0s, Learning rate: 3.520365877844011e-05\n",
            "Training Loss: 0.0121, Accuracy: 99.55%\n",
            "Validation Loss: 0.3876, Accuracy: 94.30%\n",
            "EarlyStopping counter: 1 out of 10\n",
            "Epoch [13/50], Time: 133.1s, Learning rate: 2.8027470262892444e-05\n",
            "Training Loss: 0.0091, Accuracy: 99.68%\n",
            "Validation Loss: 0.1740, Accuracy: 97.51%\n",
            "EarlyStopping counter: 2 out of 10\n",
            "Epoch [14/50], Time: 133.5s, Learning rate: 2.1404630011522593e-05\n",
            "Training Loss: 0.0003, Accuracy: 100.00%\n",
            "Validation Loss: 0.1592, Accuracy: 97.20%\n",
            "EarlyStopping counter: 3 out of 10\n",
            "Epoch [15/50], Time: 132.7s, Learning rate: 1.5498214331265904e-05\n",
            "Training Loss: 0.0001, Accuracy: 100.00%\n",
            "Validation Loss: 0.1688, Accuracy: 97.31%\n",
            "EarlyStopping counter: 4 out of 10\n",
            "Epoch [16/50], Time: 133.3s, Learning rate: 1.0453658778440109e-05\n",
            "Training Loss: 0.0000, Accuracy: 100.00%\n",
            "Validation Loss: 0.1737, Accuracy: 97.31%\n",
            "EarlyStopping counter: 5 out of 10\n",
            "Epoch [17/50], Time: 133.6s, Learning rate: 6.3951770526757955e-06\n",
            "Training Loss: 0.0000, Accuracy: 100.00%\n",
            "Validation Loss: 0.1765, Accuracy: 97.31%\n",
            "EarlyStopping counter: 6 out of 10\n",
            "Epoch [18/50], Time: 133.2s, Learning rate: 3.422702443389901e-06\n",
            "Training Loss: 0.0000, Accuracy: 100.00%\n",
            "Validation Loss: 0.1780, Accuracy: 97.31%\n",
            "EarlyStopping counter: 7 out of 10\n",
            "Epoch [19/50], Time: 133.1s, Learning rate: 1.609427140540686e-06\n",
            "Training Loss: 0.0000, Accuracy: 100.00%\n",
            "Validation Loss: 0.1789, Accuracy: 97.31%\n",
            "EarlyStopping counter: 8 out of 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 평가\n",
        "num_classes = 2\n",
        "lr = 1e-4\n",
        "today = '0828'\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(weight=chin_sagging_class_weights)\n",
        "\n",
        "best_model_state_dict = torch.load(f'{save_dir}ensemble_chin_{today}.pt', map_location=device)\n",
        "\n",
        "eval_loader = DataLoader(eval_subset, batch_size=1, shuffle=False)\n",
        "evaluator = ModelEvaluator(best_model_state_dict, eval_loader, criterion, device)\n",
        "eval_pred, eval_true = evaluator.evaluate()\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# 평가 지표 계산\n",
        "report = classification_report(eval_true, eval_pred)\n",
        "\n",
        "print(report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4H-ZyIf7_g0C",
        "outputId": "9460ff63-178f-4163-a77d-4d5250f18efb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-520f07bddb7f>:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  best_model_state_dict = torch.load(f'{save_dir}ensemble_chin_{today}.pt', map_location=device)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation Accuracy: 95.34%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.96      0.96       645\n",
            "           1       0.92      0.94      0.93       320\n",
            "\n",
            "    accuracy                           0.95       965\n",
            "   macro avg       0.95      0.95      0.95       965\n",
            "weighted avg       0.95      0.95      0.95       965\n",
            "\n"
          ]
        }
      ]
    }
  ]
}